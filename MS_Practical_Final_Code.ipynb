{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "import umap\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# ####################\n",
    "# ComfyUI & Prompt Logic\n",
    "# ####################\n",
    "\n",
    "COMFYUI_API_PATH = r\"E:\\JKU_LINZ\\Winter_25\\MS_Thesis\\Text2Image\\text2imagegen\\First_trials\\comfyuiapi\\Image_gen.json\"\n",
    "COMFYUI_SERVER = \"http://127.0.0.1:8188\"\n",
    "\n",
    "BATCH_HISTORY = []\n",
    "MAX_BATCHES = 12\n",
    "SELECTED_IMAGE_INDEX = None\n",
    "\n",
    "CLIP_HISTORY = []  # list of dicts with \"embeddings\": [np.ndarray or None, ...]\n",
    "\n",
    "ATTRIBUTE_COLOR_MAP = {\n",
    "    \"cute\":       \"#FBBF24\",\n",
    "    \"happy\":      \"#22C55E\",\n",
    "    \"ugly\":       \"#EF4444\",\n",
    "    \"sad\":        \"#3B82F6\",\n",
    "    \"joyful\":     \"#06B6D4\",\n",
    "    \"miserable\":  \"#7C3AED\",\n",
    "    \"old\":        \"#A16207\",\n",
    "    \"young\":      \"#86EFAC\",\n",
    "    \"rich\":       \"#F59E0B\",\n",
    "    \"poor\":       \"#6B7280\",\n",
    "    \"friendly\":   \"#14B8A6\",\n",
    "    \"aggressive\": \"#DC2626\",\n",
    "    \"sci-fi\":     \"#818CF8\",\n",
    "    \"mechanical\": \"#0EA5E9\",\n",
    "}\n",
    "\n",
    "def get_attr_color(attr: str) -> str:\n",
    "    return ATTRIBUTE_COLOR_MAP.get((attr or \"\").lower(), \"#AAAAAA\")\n",
    "\n",
    "# ── Scale: pushes probabilities polar.\n",
    "# Math: p = sigmoid(SCALE * delta), where delta = cosine_gap between pos/neg poles.\n",
    "# Typical CLIP cosine gap is 0.03-0.08 for clear cases.\n",
    "# SCALE=60 → delta=0.05 gives p=0.95; delta=0.08 gives p=0.99\n",
    "BIPOLAR_USE_CLIP_LOGIT_SCALE = False\n",
    "BIPOLAR_EXTRA_SCALE = 60.0  # ← raised from 20 → pushes clear cases to 0.90-0.99\n",
    "\n",
    "# ── Prompt ensembling templates \n",
    "TEXT_TEMPLATES = [\n",
    "    \"{desc}\",\n",
    "    \"a photo of {desc}\",\n",
    "    \"an illustration of {desc}\",\n",
    "    \"a close-up photo of {desc}\",\n",
    "    \"a high quality realistic photo of {desc}\",\n",
    "    \"a detailed portrait of {desc}\",\n",
    "]\n",
    "\n",
    "ANIMAL_CONTEXT_TEMPLATES: dict = {\n",
    "    \"cat\":    [\"a {desc} cat\",    \"a photo of a {desc} cat\",    \"a {desc} kitten\",    \"portrait of a {desc} cat\"],\n",
    "    \"dog\":    [\"a {desc} dog\",    \"a photo of a {desc} dog\",    \"a {desc} puppy\",     \"portrait of a {desc} dog\"],\n",
    "    \"lion\":   [\"a {desc} lion\",   \"a photo of a {desc} lion\",   \"a {desc} wild lion\", \"portrait of a {desc} lion\"],\n",
    "    \"tiger\":  [\"a {desc} tiger\",  \"a photo of a {desc} tiger\",  \"a {desc} bengal tiger\"],\n",
    "    \"bird\":   [\"a {desc} bird\",   \"a photo of a {desc} bird\",   \"a {desc} parrot or eagle\"],\n",
    "    \"dragon\": [\"a {desc} dragon\", \"a fantasy {desc} dragon\",    \"an illustration of a {desc} dragon\"],\n",
    "    \"robot\":  [\"a {desc} robot\",  \"a mechanical {desc} robot\",  \"a photo of a {desc} robot\"],\n",
    "}\n",
    "DEFAULT_ANIMAL_TEMPLATES: list = [\n",
    "    \"a {desc} animal\",\n",
    "    \"a photo of a {desc} animal\",\n",
    "    \"a {desc} creature\",\n",
    "    \"portrait of a {desc} creature\",\n",
    "]\n",
    "\n",
    "# ── Bipolar attribute pairs \n",
    "# CLIP detects VISUAL features in the image, so descriptions must target things\n",
    "# visible in a 512x512 portrait: fur texture, eye clarity, facial expression,\n",
    "# muzzle colour, body posture. Abstract concepts like \"feelings\" don't score well.\n",
    "BIPOLAR_ATTRIBUTES = {\n",
    "\n",
    "    # pair 0: CUTENESS \n",
    "    \"appearance_cuteness\": {\n",
    "        \"positive\": {\n",
    "            \"label\": \"cute\",\n",
    "            \"description\": (\n",
    "                \"extremely adorable baby animal, disproportionately huge round glassy eyes, \"\n",
    "                \"tiny soft pink button nose, ultra-fluffy plush downy coat, \"\n",
    "                \"chubby rounded puffy cheeks, small compact round body, \"\n",
    "                \"irresistibly innocent sweet gentle expression\"\n",
    "            ),\n",
    "        },\n",
    "        \"negative\": {\n",
    "            \"label\": \"ugly\",\n",
    "            \"description\": (\n",
    "                \"grotesque horribly deformed animal, severely asymmetric scarred face, \"\n",
    "                \"completely bald patchy infected scabby skin, deep sunken hollow dull eyes, \"\n",
    "                \"crooked broken yellow teeth, festering sores, diseased emaciated body\"\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # pair 1: HAPPINESS — facial expression CLIP can read from portraits\n",
    "    \"emotion_valence\": {\n",
    "        \"positive\": {\n",
    "            \"label\": \"happy\",\n",
    "            \"description\": (\n",
    "                \"visibly happy animal with wide open bright eyes, \"\n",
    "                \"mouth open showing teeth in a grin, tongue lolling out joyfully, \"\n",
    "                \"ears fully pricked forward, soft relaxed face muscles, \"\n",
    "                \"warm inviting alert expression\"\n",
    "            ),\n",
    "        },\n",
    "        \"negative\": {\n",
    "            \"label\": \"sad\",\n",
    "            \"description\": (\n",
    "                \"visibly sad animal, downcast half-closed glistening watery eyes, \"\n",
    "                \"mouth tightly shut with corners pulled down, ears pinned flat, \"\n",
    "                \"furrowed brow, heavy drooping head, deeply unhappy sorrowful face\"\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # pair 2: ALERTNESS/ENERGY — focus on FACIAL alertness, not body posture.\n",
    "    # \"Leaping mid-air\" is invisible in a sitting portrait; bright vs dull EYES work.\n",
    "    \"emotion_intensity\": {\n",
    "        \"positive\": {\n",
    "            \"label\": \"joyful\",\n",
    "            \"description\": (\n",
    "                \"extremely alert playful energetic animal, very wide sparkling bright eyes, \"\n",
    "                \"ears fully erect and pointing forward, open mouth, \"\n",
    "                \"animated lively engaged expression, whiskers spread wide, \"\n",
    "                \"intense curious attentive gaze, visibly excited\"\n",
    "            ),\n",
    "        },\n",
    "        \"negative\": {\n",
    "            \"label\": \"miserable\",\n",
    "            \"description\": (\n",
    "                \"completely lethargic depressed suffering animal, \"\n",
    "                \"half-closed glazed dull lifeless eyes, ears flat against skull, \"\n",
    "                \"tightly closed tense mouth, hunched withdrawn body, \"\n",
    "                \"vacant hollow stare, visibly exhausted and defeated\"\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # pair 3: AGE — must use FACE-VISIBLE cues that appear even in fantasy portraits.\n",
    "    # Key CLIP-visible signals: muzzle colour (grey/white = old), eye clarity,\n",
    "    # coat texture (smooth = young; coarse thin = old), facial wrinkles.\n",
    "    \"age\": {\n",
    "        \"positive\": {\n",
    "            \"label\": \"young\",\n",
    "            \"description\": (\n",
    "                \"very young juvenile animal, completely smooth clean glossy coat, \"\n",
    "                \"proportionally enormous bright clear sparkling eyes, \"\n",
    "                \"tiny soft pink nose and small round muzzle, \"\n",
    "                \"perfectly clean pure-coloured fur with no grey or white patches, \"\n",
    "                \"soft baby-like rounded facial features, fresh vibrant appearance\"\n",
    "            ),\n",
    "        },\n",
    "        \"negative\": {\n",
    "            \"label\": \"old\",\n",
    "            \"description\": (\n",
    "                \"very elderly senior animal, conspicuously white or grey fur \"\n",
    "                \"covering entire muzzle chin and forehead, \"\n",
    "                \"deeply cloudy milky opaque eyes with sunken sockets, \"\n",
    "                \"coarse sparse thinning dull coat, pronounced deep facial wrinkles, \"\n",
    "                \"thick prominent whisker pads, heavy-lidded tired aged expression\"\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # pair 4: STATUS — jewel collars / grooming vs emaciated stray\n",
    "    \"status\": {\n",
    "        \"positive\": {\n",
    "            \"label\": \"rich\",\n",
    "            \"description\": (\n",
    "                \"pampered prize show animal, perfectly groomed immaculate glossy coat, \"\n",
    "                \"ornate diamond-studded jewelled collar, bright clean white teeth, \"\n",
    "                \"posed on velvet cushion in opulent luxury setting, \"\n",
    "                \"well-fed plump healthy shiny appearance\"\n",
    "            ),\n",
    "        },\n",
    "        \"negative\": {\n",
    "            \"label\": \"poor\",\n",
    "            \"description\": (\n",
    "                \"emaciated neglected stray street animal, heavily matted tangled filthy fur, \"\n",
    "                \"no collar, prominent visible ribs and hip bones, \"\n",
    "                \"mud-caked cracked paws, open sores, scruffy unkempt appearance\"\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # pair 5: SOCIAL BEHAVIOUR\n",
    "    \"social_behavior\": {\n",
    "        \"positive\": {\n",
    "            \"label\": \"friendly\",\n",
    "            \"description\": (\n",
    "                \"gentle friendly approachable animal, soft warm inviting eyes, \"\n",
    "                \"relaxed open mouth with gentle smile, tail fully raised and wagging, \"\n",
    "                \"head tilted sideways, calm non-threatening completely relaxed posture\"\n",
    "            ),\n",
    "        },\n",
    "        \"negative\": {\n",
    "            \"label\": \"aggressive\",\n",
    "            \"description\": (\n",
    "                \"ferociously aggressive attack-ready animal, fully bared razor-sharp fangs, \"\n",
    "                \"deeply wrinkled snarling nose, wide open threatening jaw, \"\n",
    "                \"tensed crouched attack stance, ears pinned flat, raised hackles, \"\n",
    "                \"hostile intense deadly glare\"\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# ── Flat label → pair info lookup \n",
    "_LABEL_TO_BIPOLAR: dict = {}\n",
    "for _pk, _pair in BIPOLAR_ATTRIBUTES.items():\n",
    "    for _pol in (\"positive\", \"negative\"):\n",
    "        _opp = \"negative\" if _pol == \"positive\" else \"positive\"\n",
    "        _lbl = _pair[_pol][\"label\"]\n",
    "        _LABEL_TO_BIPOLAR[_lbl] = {\n",
    "            \"description\":    _pair[_pol][\"description\"],\n",
    "            \"pair_key\":       _pk,\n",
    "            \"polarity\":       _pol,\n",
    "            \"is_positive\":    (_pol == \"positive\"),\n",
    "            \"opposite_label\": _pair[_opp][\"label\"],\n",
    "            \"opposite_desc\":  _pair[_opp][\"description\"],\n",
    "        }\n",
    "\n",
    "CLIP_ATTRIBUTE_DESCRIPTIONS: dict = {\n",
    "    \"sci-fi\":     \"a sci-fi futuristic cybernetic creature, glowing implants, neon lighting\",\n",
    "    \"mechanical\": \"a fully mechanical robotic creature, exposed metal gears, rigid joints\",\n",
    "}\n",
    "for _pair in BIPOLAR_ATTRIBUTES.values():\n",
    "    for _pole in (\"positive\", \"negative\"):\n",
    "        CLIP_ATTRIBUTE_DESCRIPTIONS[_pair[_pole][\"label\"]] = _pair[_pole][\"description\"]\n",
    "\n",
    "COMPARISON_MODIFIERS = [\n",
    "    \"cute\", \"happy\", \"joyful\", \"young\", \"rich\", \"friendly\",\n",
    "    \"ugly\", \"sad\",  \"miserable\", \"old\",  \"poor\", \"aggressive\",\n",
    "]\n",
    "\n",
    "# ── Art styles optimised for 512×512 highly-detailed outputs \n",
    "# Each style adds quality boosters that help SD squeeze maximum detail at 512×512.\n",
    "ART_STYLES = {\n",
    "    \"None\": \"\",\n",
    "    \"Fantasy\": (\n",
    "        \"fantasy art, magical atmosphere, ethereal lighting, \"\n",
    "        \"masterpiece, best quality, highly detailed, sharp focus, intricate details\"\n",
    "    ),\n",
    "    \"Ultra Realistic\": (\n",
    "        \"ultra realistic, photorealistic, hyperrealistic, \"\n",
    "        \"masterpiece, best quality, 8k resolution, RAW photo, \"\n",
    "        \"highly detailed, sharp focus, HDR, subsurface scattering\"\n",
    "    ),\n",
    "    \"Watercolor\": (\n",
    "        \"watercolor painting, soft delicate colors, artistic, painterly, \"\n",
    "        \"masterpiece, best quality, highly detailed brushwork, wet-on-wet technique\"\n",
    "    ),\n",
    "    \"Sketch\": (\n",
    "        \"detailed pencil sketch, hand drawn, fine linework, crosshatching, \"\n",
    "        \"masterpiece, best quality, highly detailed, sharp lines, professional illustration\"\n",
    "    ),\n",
    "    \"Anime\": (\n",
    "        \"anime style, manga illustration, vibrant saturated colors, cel shaded, \"\n",
    "        \"masterpiece, best quality, highly detailed, sharp, studio quality animation\"\n",
    "    ),\n",
    "    \"Oil Painting\": (\n",
    "        \"oil painting, classical fine art, rich impasto brush strokes, \"\n",
    "        \"masterpiece, best quality, highly detailed, Renaissance style, dramatic lighting\"\n",
    "    ),\n",
    "    \"Digital Art\": (\n",
    "        \"digital art, concept art, trending on artstation, \"\n",
    "        \"masterpiece, best quality, highly detailed, sharp, volumetric lighting, 4k\"\n",
    "    ),\n",
    "    \"Cartoon\": (\n",
    "        \"cartoon style, stylized illustration, vibrant clean colors, \"\n",
    "        \"masterpiece, best quality, highly detailed, bold clean lines, Disney quality\"\n",
    "    ),\n",
    "    \"Cinematic\": (\n",
    "        \"cinematic photography, movie still, anamorphic lens, bokeh, \"\n",
    "        \"masterpiece, best quality, highly detailed, dramatic studio lighting, 4k\"\n",
    "    ),\n",
    "    \"Portrait\": (\n",
    "        \"professional portrait photography, studio lighting, shallow depth of field, \"\n",
    "        \"masterpiece, best quality, highly detailed, sharp focus on face, \"\n",
    "        \"DSLR 85mm lens, natural skin texture\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "BASE_ATTRIBUTES = [\n",
    "    \"cute\", \"happy\", \"ugly\", \"sad\", \"joyful\", \"sci-fi\", \"mechanical\",\n",
    "    \"miserable\", \"old\", \"young\", \"rich\", \"poor\",\n",
    "    \"friendly\", \"aggressive\", \"Custom...\"\n",
    "]\n",
    "\n",
    "BASE_SUBJECTS = [\"cat\", \"dog\", \"bird\", \"robot\", \"dragon\", \"tiger\", \"lion\", \"Custom...\"]\n",
    "\n",
    "# ── Default negative prompt optimised for 512×512 detail \n",
    "DEFAULT_NEGATIVE_PROMPT = (\n",
    "    \"text, watermark, blurry, out of focus, low quality, low resolution, \"\n",
    "    \"jpeg artifacts, noise, grain, overexposed, underexposed, \"\n",
    "    \"deformed, disfigured, bad anatomy, extra limbs, missing limbs, \"\n",
    "    \"ugly, duplicate, morbid, mutilated, poorly drawn face\"\n",
    ")\n",
    "\n",
    "# ####################\n",
    "# CLIP model (image + text)\n",
    "# ####################\n",
    "\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "_clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME)\n",
    "_clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
    "_clip_model.eval()\n",
    "\n",
    "\n",
    "####################\n",
    "# image_vec  = _clip_model.get_image_features(PIL)  → shared CLIP space\n",
    "# text_vec   = _clip_model.get_text_features(text)  → shared CLIP space\n",
    "# Cosine similarity between them \n",
    "####################\n",
    "\n",
    "def encode_image_with_clip(pil_image: Image.Image) -> np.ndarray:\n",
    "    rgb = pil_image.convert(\"RGB\")\n",
    "    inputs = _clip_processor(images=rgb, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        img_features = _clip_model.get_image_features(**inputs)\n",
    "    vec = img_features.cpu().numpy().astype(np.float32).reshape(-1)\n",
    "    vec /= (np.linalg.norm(vec) + 1e-8)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def text_modifiers_to_clip_vectors(modifiers):\n",
    "    if not modifiers:\n",
    "        return np.zeros((0, 512), dtype=np.float32)\n",
    "    descriptions = [CLIP_ATTRIBUTE_DESCRIPTIONS.get(m.lower(), m) for m in modifiers]\n",
    "    inputs = _clip_processor(\n",
    "        text=descriptions, return_tensors=\"pt\",\n",
    "        padding=True, truncation=True, max_length=77)\n",
    "    with torch.no_grad():\n",
    "        text_features = _clip_model.get_text_features(**inputs)\n",
    "    text_vecs = text_features.cpu().numpy().astype(np.float32)\n",
    "    norms = np.linalg.norm(text_vecs, axis=1, keepdims=True) + 1e-8\n",
    "    return text_vecs / norms\n",
    "\n",
    "\n",
    "def _encode_text_batch(texts: list, cache: dict) -> None:\n",
    "    missing = [t for t in texts if t not in cache]\n",
    "    if not missing:\n",
    "        return\n",
    "    inp = _clip_processor(text=missing, return_tensors=\"pt\",\n",
    "                          padding=True, truncation=True, max_length=77)\n",
    "    with torch.no_grad():\n",
    "        feats = _clip_model.get_text_features(**inp)\n",
    "    vecs = feats.cpu().numpy().astype(np.float32)\n",
    "    vecs /= (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-8)\n",
    "    for t, v in zip(missing, vecs):\n",
    "        cache[t] = v\n",
    "\n",
    "\n",
    "def _encode_ensemble(base_desc: str, cache: dict) -> np.ndarray:\n",
    "    variants = [tpl.format(desc=base_desc) for tpl in TEXT_TEMPLATES]\n",
    "    _encode_text_batch(variants, cache)\n",
    "    mat = np.stack([cache[v] for v in variants])\n",
    "    avg = mat.mean(axis=0)\n",
    "    avg /= (np.linalg.norm(avg) + 1e-8)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def _encode_ensemble_for_subject(base_desc: str, subject: str, cache: dict) -> np.ndarray:\n",
    "    species_tpls = ANIMAL_CONTEXT_TEMPLATES.get(\n",
    "        (subject or \"\").lower(), DEFAULT_ANIMAL_TEMPLATES\n",
    "    )\n",
    "    all_tpls = list(dict.fromkeys(species_tpls + TEXT_TEMPLATES))\n",
    "    variants = [tpl.format(desc=base_desc) for tpl in all_tpls]\n",
    "    _encode_text_batch(variants, cache)\n",
    "    mat = np.stack([cache[v] for v in variants])\n",
    "    avg = mat.mean(axis=0)\n",
    "    avg /= (np.linalg.norm(avg) + 1e-8)\n",
    "    return avg\n",
    "\n",
    "\n",
    "def get_effective_value(dropdown_value: str, custom_value: str):\n",
    "    custom_value = (custom_value or \"\").strip()\n",
    "    if dropdown_value == \"Custom...\":\n",
    "        return custom_value if custom_value else \"\"\n",
    "    return dropdown_value\n",
    "\n",
    "\n",
    "def load_workflow_template():\n",
    "    try:\n",
    "        with open(COMFYUI_API_PATH, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Workflow file not found at {COMFYUI_API_PATH}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def combine_prompt_with_style(base_prompt, art_style, art_style_weight):\n",
    "    if art_style and art_style != \"None\":\n",
    "        style_suffix = ART_STYLES.get(art_style, \"\")\n",
    "        if style_suffix:\n",
    "            w = round(float(art_style_weight), 1)\n",
    "            return f\"{base_prompt}, ({style_suffix}:{w})\"\n",
    "    return base_prompt\n",
    "\n",
    "\n",
    "def build_base_prompt(attribute, attribute_weight, subject):\n",
    "    if not subject:\n",
    "        subject = \"subject\"\n",
    "    if not attribute:\n",
    "        return f\"a {subject}\"\n",
    "    w = round(float(attribute_weight), 1)\n",
    "    return f\"a ({attribute}:{w}) {subject}\"\n",
    "\n",
    "\n",
    "def update_workflow_parameters(\n",
    "    workflow, positive_prompt, negative_prompt, seed, control_mode,\n",
    "    steps, cfg, sampler, scheduler, denoise, width, height, batch_size\n",
    "):\n",
    "    if not workflow:\n",
    "        return None\n",
    "    try:\n",
    "        for node_id, node_data in workflow.items():\n",
    "            if node_data.get(\"class_type\") == \"CLIPTextEncode\":\n",
    "                if \"inputs\" in node_data and positive_prompt:\n",
    "                    node_data[\"inputs\"][\"text\"] = positive_prompt\n",
    "                    break\n",
    "        clip_nodes = [k for k, v in workflow.items()\n",
    "                      if v.get(\"class_type\") == \"CLIPTextEncode\"]\n",
    "        if len(clip_nodes) > 1 and negative_prompt:\n",
    "            workflow[clip_nodes[1]][\"inputs\"][\"text\"] = negative_prompt\n",
    "        for node_id, node_data in workflow.items():\n",
    "            if node_data.get(\"class_type\") == \"KSampler\":\n",
    "                inputs = node_data.get(\"inputs\", {})\n",
    "                inputs[\"seed\"] = seed\n",
    "                inputs[\"steps\"] = steps\n",
    "                inputs[\"cfg\"] = cfg\n",
    "                inputs[\"sampler_name\"] = sampler\n",
    "                inputs[\"scheduler\"] = scheduler\n",
    "                inputs[\"denoise\"] = denoise\n",
    "        for node_id, node_data in workflow.items():\n",
    "            if node_data.get(\"class_type\") == \"EmptyLatentImage\":\n",
    "                inputs = node_data.get(\"inputs\", {})\n",
    "                inputs[\"width\"] = width\n",
    "                inputs[\"height\"] = height\n",
    "                inputs[\"batch_size\"] = batch_size\n",
    "        return workflow\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating workflow: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_images_from_comfy(prompt_id, server_address):\n",
    "    try:\n",
    "        history_url = f\"{server_address}/history/{prompt_id}\"\n",
    "        response = requests.get(history_url)\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        history = response.json()\n",
    "        if prompt_id not in history:\n",
    "            return []\n",
    "        outputs = history[prompt_id].get(\"outputs\", {})\n",
    "        images = []\n",
    "        for node_id, node_output in outputs.items():\n",
    "            if \"images\" in node_output:\n",
    "                for image_info in node_output[\"images\"]:\n",
    "                    filename = image_info[\"filename\"]\n",
    "                    subfolder = image_info.get(\"subfolder\", \"\")\n",
    "                    folder_type = image_info.get(\"type\", \"output\")\n",
    "                    image_url = f\"{server_address}/view\"\n",
    "                    params = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "                    img_response = requests.get(image_url, params=params)\n",
    "                    if img_response.status_code == 200:\n",
    "                        img = Image.open(io.BytesIO(img_response.content))\n",
    "                        images.append(img)\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving images: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_clip_embeddings_from_comfy(prompt_id, server_address):\n",
    "    try:\n",
    "        history_url = f\"{server_address}/history/{prompt_id}\"\n",
    "        response = requests.get(history_url)\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        history = response.json()\n",
    "        if prompt_id not in history:\n",
    "            return []\n",
    "        outputs = history[prompt_id].get(\"outputs\", {})\n",
    "        clip_embs = []\n",
    "        for node_id, node_output in outputs.items():\n",
    "            class_type = node_output.get(\"class_type\") or node_output.get(\"type\")\n",
    "            if class_type == \"CLIPVisionEncode\":\n",
    "                data = node_output.get(\"embeds\") or node_output.get(\"samples\") or []\n",
    "                for d in data:\n",
    "                    emb = np.array(d, dtype=np.float32)\n",
    "                    clip_embs.append(emb)\n",
    "        return clip_embs\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving CLIP embeddings:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "def wait_for_completion(prompt_id, server_address, timeout=300):\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            history_url = f\"{server_address}/history/{prompt_id}\"\n",
    "            response = requests.get(history_url)\n",
    "            if response.status_code == 200:\n",
    "                history = response.json()\n",
    "                if prompt_id in history:\n",
    "                    return True\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking status: {e}\")\n",
    "            time.sleep(1)\n",
    "    return False\n",
    "\n",
    "\n",
    "def image_to_vector_from_clip(clip_embedding: np.ndarray) -> np.ndarray:\n",
    "    vec = np.asarray(clip_embedding, dtype=np.float32).reshape(-1)\n",
    "    norm = np.linalg.norm(vec) + 1e-8\n",
    "    return vec / norm\n",
    "\n",
    "\n",
    "def compute_2d_layout_smart(images, batch_ids, subjects, clip_embeddings=None):\n",
    "    if not images or len(images) == 0:\n",
    "        return np.zeros((0, 2))\n",
    "    if clip_embeddings is not None and len(clip_embeddings) == len(images):\n",
    "        visual_feats = np.stack(\n",
    "            [image_to_vector_from_clip(emb) for emb in clip_embeddings], axis=0)\n",
    "    else:\n",
    "        visual_feats = np.stack(\n",
    "            [np.asarray(im.convert(\"RGB\").resize((64, 64)), dtype=np.float32).reshape(-1) / 255.0\n",
    "             for im in images], axis=0)\n",
    "    if visual_feats.shape[0] < 3:\n",
    "        xs = np.linspace(0.2, 0.8, visual_feats.shape[0])\n",
    "        ys = np.ones_like(xs) * 0.5\n",
    "        return np.stack([xs, ys], axis=1)\n",
    "    unique_batches = list(set(batch_ids))\n",
    "    batch_features = np.zeros((len(batch_ids), len(unique_batches)))\n",
    "    for i, batch_id in enumerate(batch_ids):\n",
    "        batch_features[i, unique_batches.index(batch_id)] = 1.0\n",
    "    unique_subjects = list(set(subjects))\n",
    "    subject_features = np.zeros((len(subjects), len(unique_subjects)))\n",
    "    for i, subject in enumerate(subjects):\n",
    "        subject_features[i, unique_subjects.index(subject)] = 1.0\n",
    "    combined_features = np.concatenate(\n",
    "        [batch_features * 25.0, subject_features * 20.0, visual_feats * 0.05], axis=1)\n",
    "    n_neighbors = min(8, len(images) - 1)\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=n_neighbors,\n",
    "                        min_dist=0.3, spread=3.0, metric=\"euclidean\")\n",
    "    emb = reducer.fit_transform(combined_features)\n",
    "    for batch in set(batch_ids):\n",
    "        batch_indices = [i for i, b in enumerate(batch_ids) if b == batch]\n",
    "        if len(batch_indices) > 1:\n",
    "            batch_center = emb[batch_indices].mean(axis=0)\n",
    "            for idx in batch_indices:\n",
    "                emb[idx] = batch_center + np.random.uniform(-0.015, 0.015, 2)\n",
    "    emb = 0.1 + 0.8 * (emb - emb.min(0)) / (emb.max(0) - emb.min(0) + 1e-8)\n",
    "    return resolve_overlaps_smart(emb, batch_ids, subjects)\n",
    "\n",
    "\n",
    "def resolve_overlaps_smart(positions, batch_ids, subjects, min_distance=90):\n",
    "    positions = positions.copy()\n",
    "    n = len(positions)\n",
    "    if n < 2:\n",
    "        return positions\n",
    "    canvas_width, canvas_height = 1000, 800\n",
    "    positions[:, 0] *= canvas_width\n",
    "    positions[:, 1] *= canvas_height\n",
    "    unique_subjects = list(set(subjects))\n",
    "    subject_to_idx = {subj: [] for subj in unique_subjects}\n",
    "    for i, subj in enumerate(subjects):\n",
    "        subject_to_idx[subj].append(i)\n",
    "    for _ in range(150):\n",
    "        moved = False\n",
    "        if len(unique_subjects) > 1:\n",
    "            cluster_centers = {subj: positions[idxs].mean(axis=0)\n",
    "                               for subj, idxs in subject_to_idx.items()}\n",
    "            subs = list(cluster_centers.keys())\n",
    "            for i in range(len(subs)):\n",
    "                for j in range(i + 1, len(subs)):\n",
    "                    sub_a, sub_b = subs[i], subs[j]\n",
    "                    diff = cluster_centers[sub_a] - cluster_centers[sub_b]\n",
    "                    dist = np.linalg.norm(diff)\n",
    "                    if 0 < dist < 150:\n",
    "                        force = (diff / dist) * (150 - dist) * 0.5\n",
    "                        for idx in subject_to_idx[sub_a]:\n",
    "                            positions[idx] += force; moved = True\n",
    "                        for idx in subject_to_idx[sub_b]:\n",
    "                            positions[idx] -= force; moved = True\n",
    "        for i in range(n):\n",
    "            forces = np.zeros(2)\n",
    "            for j in range(n):\n",
    "                if i == j: continue\n",
    "                diff = positions[i] - positions[j]\n",
    "                dist = np.linalg.norm(diff)\n",
    "                if dist <= 0: continue\n",
    "                if batch_ids[i] == batch_ids[j]:       eff = min_distance * 0.5\n",
    "                elif subjects[i] == subjects[j]:        eff = min_distance * 1.2\n",
    "                else:                                   eff = min_distance * 2.0\n",
    "                if dist < eff:\n",
    "                    forces += (diff / dist) * ((eff - dist) / eff) * 12; moved = True\n",
    "            positions[i] += forces * 0.4\n",
    "            positions[i, 0] = np.clip(positions[i, 0], 80, canvas_width - 80)\n",
    "            positions[i, 1] = np.clip(positions[i, 1], 80, canvas_height - 80)\n",
    "        if not moved: break\n",
    "    positions[:, 0] /= canvas_width\n",
    "    positions[:, 1] /= canvas_height\n",
    "    return positions\n",
    "\n",
    "\n",
    "def img_to_base64(img: Image.Image):\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"PNG\")\n",
    "    return base64.b64encode(buf.getvalue()).decode()\n",
    "\n",
    "# ####################\n",
    "# Similarity + radar chart\n",
    "# ####################\n",
    "\n",
    "\n",
    "def compute_attribute_similarity(\n",
    "    selected_image,\n",
    "    selected_image_clip,   \n",
    "    selected_attribute,\n",
    "    comparison_modifiers,\n",
    "    subject: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Bipolar Softmax scoring.\n",
    "      p(pos) = sigmoid(SCALE * (cos(img_vec, t_pos) - cos(img_vec, t_neg)))\n",
    "      p(neg) = 1 - p(pos)\n",
    "    img_vec  = CLIP image encoder  (get_image_features)\n",
    "    t_pos/neg = CLIP text encoder  (get_text_features)\n",
    "    Both live in the shared CLIP embedding space. ✓\n",
    "    \"\"\"\n",
    "    if selected_image is None or not comparison_modifiers:\n",
    "        sims, pair_pos = {}, {}\n",
    "        for mod in comparison_modifiers:\n",
    "            info = _LABEL_TO_BIPOLAR.get(mod.lower())\n",
    "            if info:\n",
    "                pk = info[\"pair_key\"]\n",
    "                if pk not in pair_pos:\n",
    "                    pair_pos[pk] = float(np.random.uniform(0.35, 0.65))\n",
    "                sims[mod] = pair_pos[pk] if info[\"is_positive\"] else 1.0 - pair_pos[pk]\n",
    "            else:\n",
    "                sims[mod] = float(np.random.uniform(0.35, 0.65))\n",
    "        return sims\n",
    "\n",
    "    # Encode PIL image → CLIP image space\n",
    "    img = encode_image_with_clip(selected_image)\n",
    "\n",
    "    scale = float(BIPOLAR_EXTRA_SCALE)\n",
    "    if BIPOLAR_USE_CLIP_LOGIT_SCALE:\n",
    "        try:\n",
    "            scale *= float(_clip_model.logit_scale.exp().detach().cpu().item())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Pre-encode all descriptions\n",
    "    cache: dict = {}\n",
    "    needed_descs = []\n",
    "    for mod in comparison_modifiers:\n",
    "        info = _LABEL_TO_BIPOLAR.get(mod.lower())\n",
    "        if info:\n",
    "            needed_descs += [info[\"description\"], info[\"opposite_desc\"]]\n",
    "        else:\n",
    "            d = CLIP_ATTRIBUTE_DESCRIPTIONS.get(mod.lower(), mod)\n",
    "            needed_descs += [d, f\"not {d}\"]\n",
    "\n",
    "    _species_tpls = ANIMAL_CONTEXT_TEMPLATES.get((subject or \"\").lower(), DEFAULT_ANIMAL_TEMPLATES)\n",
    "    _all_tpls = list(dict.fromkeys(_species_tpls + TEXT_TEMPLATES))\n",
    "    all_variants = list(dict.fromkeys(\n",
    "        tpl.format(desc=d)\n",
    "        for d in dict.fromkeys(needed_descs)\n",
    "        for tpl in _all_tpls\n",
    "    ))\n",
    "    _encode_text_batch(all_variants, cache)\n",
    "\n",
    "    pair_pos_prob: dict = {}\n",
    "    sims: dict = {}\n",
    "\n",
    "    for mod in comparison_modifiers:\n",
    "        m = mod.lower()\n",
    "        info = _LABEL_TO_BIPOLAR.get(m)\n",
    "        if info:\n",
    "            pk = info[\"pair_key\"]\n",
    "            if pk not in pair_pos_prob:\n",
    "                pos_desc = info[\"description\"] if info[\"is_positive\"] else info[\"opposite_desc\"]\n",
    "                neg_desc = info[\"opposite_desc\"] if info[\"is_positive\"] else info[\"description\"]\n",
    "                vpos = _encode_ensemble_for_subject(pos_desc, subject, cache)\n",
    "                vneg = _encode_ensemble_for_subject(neg_desc, subject, cache)\n",
    "                s1 = float(vpos @ img)\n",
    "                s2 = float(vneg @ img)\n",
    "                pair_pos_prob[pk] = float(1.0 / (1.0 + np.exp(-scale * (s1 - s2))))\n",
    "            ppos = pair_pos_prob[pk]\n",
    "            sims[mod] = ppos if info[\"is_positive\"] else (1.0 - ppos)\n",
    "        else:\n",
    "            d = CLIP_ATTRIBUTE_DESCRIPTIONS.get(m, mod)\n",
    "            v1 = _encode_ensemble_for_subject(d, subject, cache)\n",
    "            v2 = _encode_ensemble_for_subject(f\"not {d}\", subject, cache)\n",
    "            delta = float(v1 @ img) - float(v2 @ img)\n",
    "            sims[mod] = float(1.0 / (1.0 + np.exp(-scale * delta)))\n",
    "\n",
    "    return sims\n",
    "\n",
    "\n",
    "def generate_radar_chart(\n",
    "    selected_image,\n",
    "    selected_image_clip,\n",
    "    selected_attribute,\n",
    "    comparison_modifiers,\n",
    "    subject: str = \"\",\n",
    "):\n",
    "    if selected_image is None or not comparison_modifiers:\n",
    "        return (\"<div style='padding:8px;text-align:center;color:#9CA3AF;font-size:11px;'>\"\n",
    "                \"Select an image to see attribute similarity</div>\")\n",
    "\n",
    "    similarities = compute_attribute_similarity(\n",
    "        selected_image, selected_image_clip,\n",
    "        selected_attribute, comparison_modifiers, subject=subject)\n",
    "\n",
    "    n_axes = len(comparison_modifiers)\n",
    "    angles = [2 * np.pi * i / n_axes for i in range(n_axes)]\n",
    "    values = [similarities[mod] for mod in comparison_modifiers]\n",
    "    values_closed = values + [values[0]]\n",
    "    angles_closed = angles + [angles[0]]\n",
    "\n",
    "    view_width, view_height = 520, 540\n",
    "    center_x = view_width / 2\n",
    "    center_y = view_height / 2 + 22\n",
    "    radius = 180\n",
    "\n",
    "    def polar_to_cart(angle, r):\n",
    "        return (center_x + r * np.cos(angle - np.pi / 2),\n",
    "                center_y + r * np.sin(angle - np.pi / 2))\n",
    "\n",
    "    svg_parts = [f\"\"\"\n",
    "    <svg width=\"100%\" height=\"100%\" viewBox=\"0 0 {view_width} {view_height}\"\n",
    "         preserveAspectRatio=\"xMidYMid meet\"\n",
    "         style=\"background:#F8FAFC;border-radius:4px;border:1px solid #E5E7EB;\n",
    "                width:100%;min-height:440px;display:block;\">\n",
    "      <text x=\"{center_x}\" y=\"18\" text-anchor=\"middle\" font-size=\"15\" font-weight=\"700\" fill=\"#111827\">\n",
    "        Attribute Similarity Radar</text>\n",
    "      <text x=\"{center_x}\" y=\"34\" text-anchor=\"middle\" font-size=\"10\" fill=\"#6B7280\">\n",
    "        s&#x0305; = exp(s&#x2081;)/(exp(s&#x2081;)+exp(s&#x2082;)) &#x00B7; bipolar Softmax [0,1] &#x00B7; opposite pairs 180&#xB0; apart</text>\n",
    "      <text x=\"{center_x}\" y=\"50\" text-anchor=\"middle\" font-size=\"11\" fill=\"#374151\">\n",
    "        Selected: <tspan font-weight=\"700\" fill=\"#8F0E2F\">{selected_attribute}</tspan></text>\"\"\"]\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        r = radius * i / 5\n",
    "        svg_parts.append(f'<circle cx=\"{center_x}\" cy=\"{center_y}\" r=\"{r}\" fill=\"none\" stroke=\"#E5E7EB\" stroke-width=\"1\"/>')\n",
    "\n",
    "    label_radius = radius + 30\n",
    "    for i, angle in enumerate(angles):\n",
    "        xe, ye = polar_to_cart(angle, radius)\n",
    "        svg_parts.append(f'<line x1=\"{center_x}\" y1=\"{center_y}\" x2=\"{xe}\" y2=\"{ye}\" stroke=\"#CBD5E1\" stroke-width=\"1\"/>')\n",
    "        lx, ly = polar_to_cart(angle, label_radius)\n",
    "        mod = comparison_modifiers[i]\n",
    "        is_selected = mod.lower() == selected_attribute.lower()\n",
    "        fw = \"700\" if is_selected else \"500\"\n",
    "        fc = \"#8F0E2F\" if is_selected else \"#374151\"\n",
    "        lc = get_attr_color(mod) if fc == \"#374151\" else fc\n",
    "        svg_parts.append(f'<text x=\"{lx}\" y=\"{ly}\" text-anchor=\"middle\" dominant-baseline=\"middle\" font-size=\"13\" font-weight=\"{fw}\" fill=\"{lc}\">{mod}</text>')\n",
    "\n",
    "    pts = \" \".join(f\"{polar_to_cart(a,radius*v)[0]},{polar_to_cart(a,radius*v)[1]}\"\n",
    "                   for a, v in zip(angles_closed, values_closed))\n",
    "    svg_parts.append(f'<polygon points=\"{pts}\" fill=\"rgba(143,14,47,0.18)\" stroke=\"#8F0E2F\" stroke-width=\"2.2\"/>')\n",
    "\n",
    "    for angle, value in zip(angles, values):\n",
    "        x, y = polar_to_cart(angle, radius * value)\n",
    "        svg_parts.append(f'<circle cx=\"{x}\" cy=\"{y}\" r=\"5.5\" fill=\"#8F0E2F\" stroke=\"#fff\" stroke-width=\"2\"/>')\n",
    "        tx, ty = polar_to_cart(angle, radius * value + 12)\n",
    "        svg_parts.append(f'<text x=\"{tx}\" y=\"{ty}\" text-anchor=\"middle\" font-size=\"11\" font-weight=\"600\" fill=\"#1F2937\">{value:.2f}</text>')\n",
    "\n",
    "    svg_parts.append(\"</svg>\")\n",
    "    return \"\".join(svg_parts)\n",
    "\n",
    "\n",
    "def generate_sensitivity_plot(selected_image, current_attribute, current_weight):\n",
    "    if selected_image is None:\n",
    "        return \"<div style='padding:8px;text-align:center;color:#9CA3AF;font-size:11px;'>Select an image from gallery</div>\"\n",
    "    weights = np.linspace(1.0, 4.0, 13)\n",
    "    distances = 1.0 - np.exp(-0.5 * (weights - 1.0))\n",
    "    distances = distances / distances.max()\n",
    "    distances += np.random.normal(0, 0.02, len(distances))\n",
    "    distances = np.clip(distances, 0, 1)\n",
    "    vw, vh = 420, 220; pl, pr, pt, pb = 46, 20, 20, 54\n",
    "    pw, ph = vw - pl - pr, vh - pt - pb\n",
    "    xc = pl + (weights - 1.0) / 3.0 * pw\n",
    "    yc = pt + ph - distances * ph\n",
    "    path = f\"M {xc[0]},{yc[0]}\" + \"\".join(f\" L {x},{y}\" for x, y in zip(xc[1:], yc[1:]))\n",
    "    cx_ = pl + (current_weight - 1.0) / 3.0 * pw\n",
    "    cy_ = pt + ph - np.interp(current_weight, weights, distances) * ph\n",
    "    svg = f\"\"\"<svg width=\"100%\" height=\"{vh}\" viewBox=\"0 0 {vw} {vh}\"\n",
    "     preserveAspectRatio=\"xMidYMid meet\"\n",
    "     style=\"background:#F8FAFC;border-radius:4px;border:1px solid #E5E7EB;max-width:100%;\">\n",
    "  <text x=\"{vw/2}\" y=\"{pt-4}\" text-anchor=\"middle\" font-size=\"11\" font-weight=\"600\" fill=\"#111827\">Attribute Sensitivity</text>\n",
    "  <line x1=\"{pl}\" y1=\"{pt}\" x2=\"{pl}\" y2=\"{pt+ph}\" stroke=\"#CBD5E1\" stroke-width=\"1\"/>\n",
    "  <line x1=\"{pl}\" y1=\"{pt+ph}\" x2=\"{pl+pw}\" y2=\"{pt+ph}\" stroke=\"#CBD5E1\" stroke-width=\"1\"/>\n",
    "  <text x=\"{pl-6}\" y=\"{pt+ph}\" text-anchor=\"end\" font-size=\"8\" fill=\"#9CA3AF\">0.0</text>\n",
    "  <text x=\"{pl-6}\" y=\"{pt+ph*0.75}\" text-anchor=\"end\" font-size=\"8\" fill=\"#9CA3AF\">0.25</text>\n",
    "  <text x=\"{pl-6}\" y=\"{pt+ph*0.5}\" text-anchor=\"end\" font-size=\"8\" fill=\"#9CA3AF\">0.5</text>\n",
    "  <text x=\"{pl-6}\" y=\"{pt+ph*0.25}\" text-anchor=\"end\" font-size=\"8\" fill=\"#9CA3AF\">0.75</text>\n",
    "  <text x=\"{pl-6}\" y=\"{pt}\" text-anchor=\"end\" font-size=\"8\" fill=\"#9CA3AF\">1.0</text>\n",
    "  <text x=\"{pl-28}\" y=\"{pt+ph/2}\" text-anchor=\"middle\" font-size=\"9\" fill=\"#6B7280\"\n",
    "        transform=\"rotate(-90,{pl-28},{pt+ph/2})\">Embedding Distance</text>\n",
    "  <text x=\"{pl}\" y=\"{pt+ph+10}\" text-anchor=\"middle\" font-size=\"8\" fill=\"#9CA3AF\">1.0</text>\n",
    "  <text x=\"{pl+pw/2}\" y=\"{pt+ph+10}\" text-anchor=\"middle\" font-size=\"8\" fill=\"#9CA3AF\">2.5</text>\n",
    "  <text x=\"{pl+pw}\" y=\"{pt+ph+10}\" text-anchor=\"middle\" font-size=\"8\" fill=\"#9CA3AF\">4.0</text>\n",
    "  <text x=\"{pl+pw/2}\" y=\"{pt+ph+24}\" text-anchor=\"middle\" font-size=\"9\" fill=\"#6B7280\">Attribute Strength</text>\n",
    "  <path d=\"{path}\" fill=\"none\" stroke=\"#8F0E2F\" stroke-width=\"2.0\"/>\n",
    "  <circle cx=\"{cx_}\" cy=\"{cy_}\" r=\"4.5\" fill=\"#8F0E2F\" stroke=\"#fff\" stroke-width=\"1.5\"/>\n",
    "  <text x=\"{vw/2}\" y=\"{vh-22}\" text-anchor=\"middle\" font-size=\"9\" fill=\"#374151\">How much does the latent representation move when this attribute changes?</text>\n",
    "  <text x=\"{vw/2}\" y=\"{vh-10}\" text-anchor=\"middle\" font-size=\"9\" fill=\"#4B5563\">Sensitivity(s) = || z(s) - z(s-&#916;) ||,  z = image/latent embedding</text>\n",
    "</svg>\"\"\"\n",
    "    return svg\n",
    "\n",
    "# ####################\n",
    "# Cluster layout & gallery\n",
    "# ####################\n",
    "\n",
    "\n",
    "def generate_cluster_html():\n",
    "    seen_attrs: dict = {}\n",
    "    for batch in BATCH_HISTORY:\n",
    "        a = batch.get(\"attribute\", \"\")\n",
    "        if a and a not in seen_attrs:\n",
    "            seen_attrs[a] = get_attr_color(a)\n",
    "\n",
    "    def _bcolor(attr: str) -> str:\n",
    "        return seen_attrs.get(attr, get_attr_color(attr))\n",
    "\n",
    "    all_images, all_batch_ids, all_subjects, all_clip_embs = [], [], [], []\n",
    "    for batch_idx, batch in enumerate(BATCH_HISTORY):\n",
    "        clip_batch = CLIP_HISTORY[batch_idx][\"embeddings\"] if batch_idx < len(CLIP_HISTORY) else None\n",
    "        for i, thumb in enumerate(batch[\"thumbs\"]):\n",
    "            if thumb is not None:\n",
    "                img_item = {\n",
    "                    \"img\": batch[\"full\"][i], \"thumb\": thumb,\n",
    "                    \"prompt\": batch[\"prompt\"], \"seed\": batch[\"seed\"],\n",
    "                    \"subject\": batch[\"subject\"], \"attribute\": batch.get(\"attribute\", \"\"),\n",
    "                    \"clip\": clip_batch[i] if clip_batch and i < len(clip_batch) else None,\n",
    "                }\n",
    "                all_clip_embs.append(img_item[\"clip\"])\n",
    "                all_images.append(img_item)\n",
    "                all_batch_ids.append(batch_idx)\n",
    "                all_subjects.append(batch[\"subject\"])\n",
    "\n",
    "    if not all_images:\n",
    "        return \"<div style='padding:8px;text-align:center;color:#9CA3AF;font-size:12px;'>No images yet</div>\"\n",
    "\n",
    "    imgs_only = [item[\"img\"] for item in all_images]\n",
    "    clip_for_layout = None if any(e is None for e in all_clip_embs) else all_clip_embs\n",
    "    positions = compute_2d_layout_smart(imgs_only, all_batch_ids, all_subjects, clip_embeddings=clip_for_layout)\n",
    "\n",
    "    html_parts = [\"\"\"<style>\n",
    "    .image-browser{position:relative;width:100%;height:calc(100vh - 40px);\n",
    "        background:linear-gradient(135deg,#F4F6FB 0%,#EEF0F8 100%);\n",
    "        border-radius:6px;overflow:hidden;box-shadow:inset 0 2px 10px rgba(0,0,0,0.05);}\n",
    "    .image-item{position:absolute;transition:all 0.3s cubic-bezier(0.4,0,0.2,1);\n",
    "        filter:drop-shadow(0 4px 8px rgba(0,0,0,0.1));z-index:1;}\n",
    "    .image-item:hover{transform:translate(-50%,-50%) scale(1.2)!important;\n",
    "        filter:drop-shadow(0 8px 16px rgba(0,0,0,0.2));z-index:100!important;}\n",
    "    .image-item.selected{transform:translate(-50%,-50%) scale(1.15)!important;\n",
    "        filter:drop-shadow(0 6px 20px rgba(143,14,47,0.5));z-index:99!important;}\n",
    "    .image-item img{width:70px;height:70px;border-radius:8px;border:3px solid #CCC;\n",
    "        object-fit:cover;background:white;}\n",
    "    .image-item.selected img{border-width:4px!important;\n",
    "        box-shadow:0 0 0 2px #fff,0 0 0 5px rgba(0,0,0,0.3)!important;}\n",
    "    .image-label{position:absolute;bottom:-22px;left:50%;transform:translateX(-50%);\n",
    "        background:rgba(255,255,255,0.95);color:#111827;padding:2px 7px;border-radius:4px;\n",
    "        font-size:10px;font-weight:600;white-space:nowrap;opacity:0;transition:opacity 0.3s;\n",
    "        pointer-events:none;font-family:monospace;box-shadow:0 2px 8px rgba(0,0,0,0.1);\n",
    "        border:1px solid #E5E7EB;}\n",
    "    .image-item:hover .image-label{opacity:1;}\n",
    "    </style><div class=\"image-browser\" id=\"imageBrowser\">\"\"\"]\n",
    "\n",
    "    for idx, (item, pos) in enumerate(zip(all_images, positions)):\n",
    "        x_pct, y_pct = pos[0] * 100, pos[1] * 100\n",
    "        b64 = img_to_base64(item[\"thumb\"].resize((70, 70), Image.Resampling.LANCZOS))\n",
    "        sel = \"selected\" if SELECTED_IMAGE_INDEX == idx else \"\"\n",
    "        html_parts.append(\n",
    "            f'<div class=\"image-item {sel}\" id=\"img-item-{idx}\" data-index=\"{idx}\" '\n",
    "            f'style=\"left:{x_pct}%;top:{y_pct}%;transform:translate(-50%,-50%);\">'\n",
    "            f'<img src=\"data:image/png;base64,{b64}\" alt=\"Image {idx}\" '\n",
    "            f'style=\"border:3px solid {_bcolor(item.get(\"attribute\",\"\"))}!important;\">'\n",
    "            f'<div class=\"image-label\">{item.get(\"attribute\",\"\")} &middot; {item[\"seed\"]}</div></div>'\n",
    "        )\n",
    "    if seen_attrs:\n",
    "        pills = \"\".join(\n",
    "            f'<div style=\"display:inline-flex;align-items:center;gap:5px;margin:0 4px 0 0;'\n",
    "            f'padding:3px 9px;background:rgba(255,255,255,0.92);border:1.5px solid {col};'\n",
    "            f'border-radius:20px;box-shadow:0 1px 4px rgba(0,0,0,0.10);\">'\n",
    "            f'<div style=\"width:10px;height:10px;border-radius:50%;background:{col};flex-shrink:0;\"></div>'\n",
    "            f'<span style=\"font-size:11px;font-weight:600;color:#111827;white-space:nowrap;\">{attr}</span></div>'\n",
    "            for attr, col in seen_attrs.items()\n",
    "        )\n",
    "        html_parts.append(\n",
    "            f'<div style=\"position:absolute;top:8px;left:50%;transform:translateX(-50%);'\n",
    "            f'display:flex;flex-wrap:wrap;justify-content:center;gap:4px;'\n",
    "            f'z-index:200;max-width:95%;pointer-events:none;\">{pills}</div>'\n",
    "        )\n",
    "    html_parts.append(\"</div>\")\n",
    "    return \"\".join(html_parts)\n",
    "\n",
    "\n",
    "def create_gallery_data():\n",
    "    gallery_images = []\n",
    "    for batch_idx, batch in enumerate(BATCH_HISTORY):\n",
    "        for i, thumb in enumerate(batch[\"thumbs\"]):\n",
    "            if thumb is not None:\n",
    "                gallery_images.append((batch[\"full\"][i], f\"Seed: {batch['seed']} | {batch['subject']}\"))\n",
    "    return gallery_images\n",
    "\n",
    "\n",
    "def on_gallery_select(evt: gr.SelectData):\n",
    "    global SELECTED_IMAGE_INDEX\n",
    "    index = evt.index\n",
    "    all_images = []\n",
    "    for batch_idx, batch in enumerate(BATCH_HISTORY):\n",
    "        clip_batch = CLIP_HISTORY[batch_idx][\"embeddings\"] if batch_idx < len(CLIP_HISTORY) else None\n",
    "        for i, thumb in enumerate(batch[\"thumbs\"]):\n",
    "            if thumb is not None:\n",
    "                img_item = {\n",
    "                    \"img\": batch[\"full\"][i], \"prompt\": batch[\"prompt\"],\n",
    "                    \"seed\": batch[\"seed\"], \"attribute\": batch.get(\"attribute\", \"cute\"),\n",
    "                    \"weight\": batch.get(\"weight\", 2.0), \"subject\": batch.get(\"subject\", \"dog\"),\n",
    "                    \"clip\": clip_batch[i] if clip_batch and i < len(clip_batch) else None,\n",
    "                }\n",
    "                all_images.append(img_item)\n",
    "\n",
    "    empty_plot = \"<div style='padding:8px;text-align:center;color:#9CA3AF;font-size:11px;'>Select an image from gallery</div>\"\n",
    "\n",
    "    if 0 <= index < len(all_images):\n",
    "        SELECTED_IMAGE_INDEX = index\n",
    "        item = all_images[index]\n",
    "        selected_attr = item[\"attribute\"]\n",
    "        dynamic_modifiers = list(COMPARISON_MODIFIERS)\n",
    "        if selected_attr and selected_attr.lower() not in [m.lower() for m in dynamic_modifiers]:\n",
    "            dynamic_modifiers.append(selected_attr)\n",
    "        sensitivity_html = generate_sensitivity_plot(item[\"img\"], selected_attr, item[\"weight\"])\n",
    "        radar_html = generate_radar_chart(\n",
    "            item[\"img\"], item[\"clip\"], selected_attr, dynamic_modifiers,\n",
    "            subject=item.get(\"subject\", \"\"))\n",
    "        return f\"{item['prompt']}\\nSeed: {item['seed']}\", generate_cluster_html(), sensitivity_html, radar_html\n",
    "\n",
    "    return \"\", generate_cluster_html(), empty_plot, empty_plot\n",
    "\n",
    "# ####################\n",
    "# Generation pipeline\n",
    "# ####################\n",
    "\n",
    "\n",
    "def generate_image(\n",
    "    attribute_dropdown, attribute_custom, subject_dropdown, subject_custom,\n",
    "    attribute_weight, art_style, art_style_weight, negative_prompt,\n",
    "    seed, control_mode, steps, cfg, sampler, scheduler, denoise,\n",
    "    width, height, batch_size, progress=gr.Progress(),\n",
    "):\n",
    "    global SELECTED_IMAGE_INDEX\n",
    "    effective_attribute = get_effective_value(attribute_dropdown, attribute_custom)\n",
    "    effective_subject = get_effective_value(subject_dropdown, subject_custom)\n",
    "    empty_plot = \"<div style='padding:8px;text-align:center;color:#9CA3AF;font-size:11px;'>Select an image from gallery</div>\"\n",
    "    if not effective_attribute:\n",
    "        return (\"Error: Please provide an attribute\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "    if not effective_subject:\n",
    "        return (\"Error: Please provide a subject\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "    progress(0, desc=\"Loading workflow...\")\n",
    "    base_prompt = build_base_prompt(effective_attribute, attribute_weight, effective_subject)\n",
    "    combined_prompt = combine_prompt_with_style(base_prompt, art_style, art_style_weight)\n",
    "    workflow = load_workflow_template()\n",
    "    if not workflow:\n",
    "        return (\"Error: Could not load workflow template\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "    if control_mode == \"randomize\":\n",
    "        import random; seed = random.randint(0, 2**32 - 1)\n",
    "    progress(0.2, desc=\"Updating parameters...\")\n",
    "    updated_workflow = update_workflow_parameters(\n",
    "        workflow, combined_prompt, negative_prompt, seed, control_mode,\n",
    "        steps, cfg, sampler, scheduler, denoise, width, height, batch_size)\n",
    "    if not updated_workflow:\n",
    "        return (\"Error: Could not update workflow parameters\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "    try:\n",
    "        progress(0.3, desc=\"Queuing prompt...\")\n",
    "        response = requests.post(f\"{COMFYUI_SERVER}/prompt\", json={\"prompt\": updated_workflow}, timeout=30)\n",
    "        if response.status_code != 200:\n",
    "            return (f\"Error: API returned status {response.status_code}\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "        result = response.json()\n",
    "        prompt_id = result.get(\"prompt_id\")\n",
    "        if not prompt_id:\n",
    "            return (\"Error: No prompt_id returned\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "        progress(0.4, desc=f\"Generating... ID: {prompt_id}\")\n",
    "        if not wait_for_completion(prompt_id, COMFYUI_SERVER):\n",
    "            return (\"Error: Generation timeout\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "        progress(0.9, desc=\"Retrieving images...\")\n",
    "        images = get_images_from_comfy(prompt_id, COMFYUI_SERVER)\n",
    "        clip_embs = get_clip_embeddings_from_comfy(prompt_id, COMFYUI_SERVER)\n",
    "        if not images:\n",
    "            return (\"Error: No images generated\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "        images = images[:batch_size]; actual_count = len(images)\n",
    "        while len(images) < batch_size: images.append(None)\n",
    "        if clip_embs and len(clip_embs) >= actual_count:\n",
    "            clip_embs = clip_embs[:batch_size]\n",
    "            while len(clip_embs) < batch_size: clip_embs.append(None)\n",
    "        else:\n",
    "            clip_embs = [None] * batch_size\n",
    "        thumbs = [im.resize((70, 70), Image.Resampling.LANCZOS) if im else None for im in images]\n",
    "        BATCH_HISTORY.append({\n",
    "            \"thumbs\": thumbs, \"full\": images, \"prompt\": combined_prompt,\n",
    "            \"seed\": seed, \"subject\": effective_subject,\n",
    "            \"attribute\": effective_attribute, \"weight\": attribute_weight})\n",
    "        CLIP_HISTORY.append({\"embeddings\": clip_embs})\n",
    "        if len(BATCH_HISTORY) > MAX_BATCHES:\n",
    "            removed = BATCH_HISTORY.pop(0); CLIP_HISTORY.pop(0) if CLIP_HISTORY else None\n",
    "            removed_count = len([t for t in removed[\"thumbs\"] if t is not None])\n",
    "            if SELECTED_IMAGE_INDEX is not None:\n",
    "                SELECTED_IMAGE_INDEX = max(-1, SELECTED_IMAGE_INDEX - removed_count)\n",
    "        progress(1.0, desc=\"Complete!\")\n",
    "        total_images = sum(len([t for t in b[\"thumbs\"] if t]) for b in BATCH_HISTORY)\n",
    "        return (f\"Generated {actual_count} image(s)!  ||  Total: {total_images}\",\n",
    "                generate_cluster_html(), create_gallery_data(), \"\", empty_plot, empty_plot)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return (\"Error: Could not connect to ComfyUI server\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "    except Exception as e:\n",
    "        import traceback; traceback.print_exc()\n",
    "        return (f\"Error: {str(e)}\", generate_cluster_html(), [], \"\", empty_plot, empty_plot)\n",
    "\n",
    "\n",
    "def update_preview(attribute_dropdown, attribute_custom, attribute_weight,\n",
    "                   subject_dropdown, subject_custom, art_style, art_style_weight):\n",
    "    eff_attr = get_effective_value(attribute_dropdown, attribute_custom)\n",
    "    eff_subj = get_effective_value(subject_dropdown, subject_custom)\n",
    "    if not eff_attr or not eff_subj: return \"\"\n",
    "    return combine_prompt_with_style(build_base_prompt(eff_attr, attribute_weight, eff_subj), art_style, art_style_weight)\n",
    "\n",
    "\n",
    "def toggle_attribute_custom(v): return gr.update(visible=(v == \"Custom...\"))\n",
    "def toggle_subject_custom(v):   return gr.update(visible=(v == \"Custom...\"))\n",
    "\n",
    "# ####################\n",
    "# Frontend with Gradio\n",
    "# ####################\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(), css=\"\"\"\n",
    "html,body{margin:0!important;padding:0!important;height:100%!important;overflow-x:hidden!important;overflow-y:auto!important;}\n",
    ".gradio-container{max-width:100vw!important;width:100vw!important;min-height:100vh!important;height:auto!important;background-color:#EDEBEB!important;padding:0!important;margin:0!important;overflow:visible!important;}\n",
    ".panel{background:#FFFFFF!important;border-radius:0!important;border-right:1px solid #E5E7EB!important;padding:4px 8px!important;margin:0!important;max-height:calc(100vh - 32px)!important;height:auto!important;overflow-y:auto!important;}\n",
    ".panel:last-child{border-right:none!important;}\n",
    ".panel h3{display:none!important;}\n",
    ".panel label{font-size:13px!important;font-weight:500!important;margin-bottom:0!important;color:#111827!important;}\n",
    ".gradio-container .panel .wrap{margin-bottom:2px!important;}\n",
    ".panel .accordion{margin-bottom:2px!important;}\n",
    ".panel .wrap:has(button){margin-top:0!important;margin-bottom:2px!important;}\n",
    ".panel input,.panel select,.panel textarea{font-size:13px!important;background:#FFFFFF!important;border:1px solid #E5E7EB!important;color:#111827!important;padding:3px 7px!important;}\n",
    "button{background:#302F2F!important;color:white!important;font-weight:600!important;border-radius:6px!important;font-size:14px!important;padding-top:4px!important;padding-bottom:4px!important;}\n",
    "button:hover{background:#252424!important;transform:translateY(-1px);box-shadow:0 4px 12px rgba(0,0,0,0.25)!important;}\n",
    ".row{margin:0!important;gap:0!important;}.col{padding:0!important;margin:0!important;}\n",
    ".panel .tabs{margin-bottom:0!important;padding-bottom:0!important;}\n",
    ".panel .tabitem{padding-top:2px!important;padding-bottom:2px!important;}\n",
    "input[type=\"range\"]{accent-color:#302F2F!important;}\n",
    "input[type=\"range\"]::-webkit-slider-thumb{background:#302F2F!important;border:2px solid #302F2F!important;}\n",
    "input[type=\"range\"]::-webkit-slider-runnable-track{background:#302F2F22!important;}\n",
    "input[type=\"range\"]::-moz-range-thumb{background:#302F2F!important;border:2px solid #302F2F!important;}\n",
    "input[type=\"range\"]::-moz-range-track{background:#302F2F22!important;}\n",
    ".accordion{border-radius:4px!important;margin-top:4px!important;background:#F8FAFC!important;border:1px solid #E5E7EB!important;}\n",
    ".accordion>div:first-child{padding:4px 8px!important;}\n",
    ".compact-plot{margin:0!important;padding:0!important;width:100%!important;overflow:hidden!important;background:#F8FAFC!important;}\n",
    ".compact-plot svg{max-width:100%!important;height:auto!important;}\n",
    "\"\"\") as demo:\n",
    "    with gr.Row(elem_classes=\"row\"):\n",
    "        with gr.Column(scale=3, elem_classes=\"panel\"):\n",
    "            attribute_dropdown = gr.Dropdown(label=\"Attribute\", choices=BASE_ATTRIBUTES, value=\"cute\")\n",
    "            attribute_custom   = gr.Textbox(label=\"Custom Attribute\", placeholder=\"e.g., playful, mystical\", visible=False)\n",
    "            attribute_weight   = gr.Slider(label=\"Attribute Strength\", minimum=0.1, maximum=4.0, value=2.0, step=0.1)\n",
    "            subject_dropdown   = gr.Dropdown(label=\"Subject\", choices=BASE_SUBJECTS, value=\"dog\")\n",
    "            subject_custom     = gr.Textbox(label=\"Custom Subject\", placeholder=\"e.g., wolf, spaceship\", visible=False)\n",
    "            positive_prompt_preview = gr.Textbox(label=\"Auto-generated Prompt\", interactive=False, lines=2)\n",
    "            art_style          = gr.Dropdown(label=\"Art Style\", choices=list(ART_STYLES.keys()), value=\"Fantasy\")\n",
    "            art_style_weight   = gr.Slider(label=\"Art Style Strength\", minimum=0.1, maximum=2.0, value=1.0, step=0.1)\n",
    "            negative_prompt    = gr.Textbox(label=\"Negative Prompt\", lines=2, value=DEFAULT_NEGATIVE_PROMPT)\n",
    "            with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "                seed         = gr.Number(label=\"Seed\", value=1093030236344156, precision=0)\n",
    "                control_mode = gr.Dropdown(label=\"Control\", choices=[\"randomize\",\"fixed\",\"increment\",\"decrement\"], value=\"randomize\")\n",
    "                steps        = gr.Slider(label=\"Steps\", minimum=1, maximum=150, value=30, step=1)\n",
    "                cfg          = gr.Slider(label=\"CFG\", minimum=0, maximum=20, value=7.0, step=0.1)\n",
    "                sampler      = gr.Dropdown(label=\"Sampler\", choices=[\"euler\",\"euler_ancestral\",\"dpm_2\"], value=\"euler\")\n",
    "                scheduler    = gr.Dropdown(label=\"Scheduler\", choices=[\"sgm_uniform\",\"normal\",\"karras\"], value=\"sgm_uniform\")\n",
    "                denoise      = gr.Slider(label=\"Denoise\", minimum=0, maximum=1, value=1.0, step=0.01)\n",
    "                width        = gr.Slider(label=\"Width\",  minimum=512, maximum=1024, value=512, step=64)\n",
    "                height       = gr.Slider(label=\"Height\", minimum=512, maximum=1024, value=512, step=64)\n",
    "                batch_size   = gr.Slider(label=\"Batch Size\", minimum=1, maximum=4, value=4, step=1)\n",
    "            generate_btn = gr.Button(\"Generate Images\", variant=\"primary\", size=\"lg\")\n",
    "            status_text  = gr.Textbox(label=\"Status\", interactive=False, lines=1)\n",
    "\n",
    "        with gr.Column(scale=8, elem_classes=\"panel col\"):\n",
    "            with gr.Tabs():\n",
    "                with gr.Tab(\"UMAP View\"):\n",
    "                    cluster_canvas = gr.HTML(value=\"<div style='padding:8px;text-align:center;color:#9CA3AF;font-size:12px;'>Generate images to see spatial clustering</div>\")\n",
    "                with gr.Tab(\"Gallery\"):\n",
    "                    image_gallery  = gr.Gallery(label=\"All Generated Images\", show_label=False, columns=6, rows=2, height=600, object_fit=\"cover\")\n",
    "\n",
    "        with gr.Column(scale=4, elem_classes=\"panel col\"):\n",
    "            selected_prompt  = gr.Textbox(label=\"Prompt & Seed\", lines=3, interactive=False)\n",
    "            sensitivity_plot = gr.HTML(value=\"<div style='padding:8px;text-align:center;color:#9CA3AF;font-size:11px;'>Select an image from gallery</div>\", elem_classes=\"compact-plot\")\n",
    "            radar_plot       = gr.HTML(value=\"<div style='padding:8px;text-align:center;color:#9CA3AF;font-size:11px;'>Select an image to see attribute similarity</div>\", elem_classes=\"compact-plot\")\n",
    "\n",
    "    attribute_dropdown.change(fn=toggle_attribute_custom, inputs=[attribute_dropdown], outputs=[attribute_custom])\n",
    "    subject_dropdown.change(fn=toggle_subject_custom, inputs=[subject_dropdown], outputs=[subject_custom])\n",
    "    for comp in [attribute_dropdown, attribute_custom, attribute_weight,\n",
    "                 subject_dropdown, subject_custom, art_style, art_style_weight]:\n",
    "        comp.change(fn=update_preview,\n",
    "                    inputs=[attribute_dropdown, attribute_custom, attribute_weight,\n",
    "                            subject_dropdown, subject_custom, art_style, art_style_weight],\n",
    "                    outputs=[positive_prompt_preview])\n",
    "    generate_btn.click(fn=generate_image,\n",
    "        inputs=[attribute_dropdown, attribute_custom, subject_dropdown, subject_custom,\n",
    "                attribute_weight, art_style, art_style_weight, negative_prompt,\n",
    "                seed, control_mode, steps, cfg, sampler, scheduler, denoise,\n",
    "                width, height, batch_size],\n",
    "        outputs=[status_text, cluster_canvas, image_gallery, selected_prompt, sensitivity_plot, radar_plot])\n",
    "    image_gallery.select(fn=on_gallery_select,\n",
    "        outputs=[selected_prompt, cluster_canvas, sensitivity_plot, radar_plot])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
