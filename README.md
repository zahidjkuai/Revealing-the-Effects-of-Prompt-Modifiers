# Revealing the Effects of Prompt Modifiers in Image Generation
**Abstract:**
This work presents a practical experiment platform for controlled textto-image generation and semantic embedding analysis. Images are generated via a locally hosted Stable Diffusion model through the ComfyUI API, with fully controlled parameters — including seed, CFG scale, sampler, scheduler, and resolution. Prompt modifiers such as happy, rich, or aggressive are applied with explicit numerical weighting using the syntax a (attribute:strength) subject, enabling systematic sweeps of attribute intensity from 0.1 to 4.0. Multiple art styles can be layered with independent strength parameters, providing fine-grained compositional control over generated content. To compare images across different parameter settings and prompts, all outputs are encoded into a shared 512-dimensional CLIP embedding space. These embeddings are projected into 2D via UMAP, clustering images by subject and batch origin to visually reveal how different prompt configurations shift the latent distribution. For single-image analysis, alignment with semantic
modifiers is quantified using a bipolar softmax scoring mechanism over CLIP cosine similarities. Scores across all tracked attributes are rendered simultaneously in a radar chart, aiming to reveal how the dominant prompt modifier correlates with neighboring semantic attributes — exposing whether, for example, a happy image inherently scores high on joyful and friendly. The key strength of this tool lies in bridging prompt engineering and latent space interpretation: it empowers researchers to systematically audit how diffusion models encode, entangle, and respond to semantic attributes — making it a practical instrument for studying prompt sensitivity, attribute interference, and embedding structure in generative image models.

----------------------------------------------------------------------------------------------------
![Screenshot_2026_02_22-12](https://github.com/user-attachments/assets/9bbd77cb-40b7-46b2-82b5-cb7d59336f84)
**Figure 1 – Attribute‑guided prompt interface and UMAP Cluster view.**  
The interface offers structured prompt control: users pick a semantic attribute with a strength slider, a subject, and an art style with its own strength, while a negative prompt filters artifacts and advanced options (seed, sampler, scheduler, resolution) ensure full reproducibility. On the right, a UMAP view projects all generated images into a 2D latent space, clustering color‑coded thumbnails by attribute and subject to reveal how different prompt settings shift semantics at a glance.

----------------------------------------------------------------------------------------------------
For both the cute cat (figure 3) and cute dog (figure 3) examples, SDXL decodes the modifier cute almost purely as an appearance cue: large eyes, soft fur, compact proportions, clean and well‑lit presentation, but with a neutral closed‑mouth facial expression. This means the images look extremely cute yet emotionally calm, not explicitly happy or sad, and the elevated sad score in the radar plot reflects CLIP’s difficulty distinguishing calm from mildly sad rather than a truly sad generation; the (figure 4) confirms this by showing that calm‑like expressions sit in a region where our radar has no dedicated calm attribute, exposing a weakness of CLIP’s emotional sensitivity rather than a failure of the diffusion model. 

![Screenshot_2026_02_22-1](https://github.com/user-attachments/assets/57900517-f4e7-47b0-a42b-d8e7e0a34088)
**Figure 2 – Attribute‑guided a (cute:3.4) dog image generation and its Cosine Similarity Radar Chart of the Attributes** 
![Screenshot_2026_02_22-3](https://github.com/user-attachments/assets/0e7ffb7f-0a8f-4500-9597-5ed114dfa8c3)
**Figure 3 – Attribute‑guided a (cute:3.4) cat image generation and its Cosine Similarity Radar Chart of the Attributes** 
![Screenshot_2026_02_22-5](https://github.com/user-attachments/assets/d2a443d4-e254-4f87-b6cf-fe2ece6d93d3)
**Figure 4 – Attribute‑guided a (cute:3.4) cat image generation and its Cosine Similarity Radar Chart of the Attributes**

This verifies that, SDXL appears subject‑independent in the “cute cat” and “cute dog” experiments because the cute modifier is encoded as a global direction in the CLIP text embedding space that does not depend on whether the subject token is “cat” or “dog”. During conditioning, this attribute direction steers the diffusion process toward a consistent set of appearance features—large eyes, soft fur, compact proportions, clean grooming—while the subject token only determines species‑specific anatomy and texture. As a result, both animals share almost identical pose and expression: a calm, neutral, closed‑mouth look that CLIP sometimes misinterprets as partly sad, revealing that SDXL separates what is generated (subject) from how it looks (modifier), and that the cute direction itself carries no emotional valence across subjects.

----------------------------------------------------------------------------------------------------
